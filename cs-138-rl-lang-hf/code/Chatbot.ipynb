{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8347f9b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1bd66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import codecs \n",
    "import csv \n",
    "import torch\n",
    "from torch import optim \n",
    "from src import * # This loads all the models, utilities and vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the device to use \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d582d8",
   "metadata": {},
   "source": [
    "## Data Loading and Preproecessing - DailyDialog Coprus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Dataset specific globals\n",
    "CORPUS_NAME = 'dailydialog'\n",
    "CORPUS_PATH = os.path.join('data',CORPUS_NAME)\n",
    "CORPUS_TEXT_FILE_PATH = os.path.join(CORPUS_PATH,\"dialogues_text.txt\")\n",
    "CORPUS_FORMATTED_FILE_PATH = os.path.join(\n",
    "    CORPUS_PATH, 'formatted_dialogues_text.txt')\n",
    "DELIMITER = str(codecs.decode('\\t','unicode_escape'))\n",
    "\n",
    "print(CORPUS_TEXT_FILE_PATH)\n",
    "print(CORPUS_FORMATTED_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f288bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of the dataset paths symlink is working\n",
    "print(os.path.isfile(CORPUS_TEXT_FILE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44158c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dailydialog dataset: https://www.aclweb.org/anthology/I17-1099/\n",
    "printLines(CORPUS_TEXT_FILE_PATH,n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8842606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the data file\n",
    "print('Loading conversations from {}'.format(CORPUS_TEXT_FILE_PATH))\n",
    "conversations = loadLines(CORPUS_TEXT_FILE_PATH)\n",
    "print(\"Writing new formatted file: {}...\".format(CORPUS_FORMATTED_FILE_PATH))\n",
    "datafile = CORPUS_FORMATTED_FILE_PATH\n",
    "with open(datafile,'w',encoding ='utf-8') as f: \n",
    "    writer = csv.writer(f, delimiter = DELIMITER, lineterminator='\\n')\n",
    "    for pair in utils.extractSentencePairs(conversations):\n",
    "        writer.writerow(pair)\n",
    "print(\"Written formatted file: {}\".format(datafile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b935d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the formatted data \n",
    "printLines(datafile, n = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d4c622",
   "metadata": {},
   "source": [
    "## General Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64877452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -- GLOBALS\n",
    "datafile = CORPUS_FORMATTED_FILE_PATH\n",
    "# List of dull responses \n",
    "DULL_RESPONSES = [\"I do not know what you are talking about.\", \"I do not know.\", \n",
    " \"You do not know.\", \"You know what I mean.\", \"I know what you mean.\", \n",
    " \"You know what I am saying.\", \"You do not know anything.\"]\n",
    "\n",
    "\n",
    "MAX_UTTERANCE_LENGTH = 10 # Maximum length of a sentence \n",
    "MIN_TRIM_COUNT  = 0 # Words that occur less than this are trimmed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02b83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating vocabulary and pairs...')\n",
    "voc, pairs = utils.loadPrepareDataset(\n",
    "    CORPUS_NAME, datafile, DULL_RESPONSES, MIN_TRIM_COUNT, MAX_UTTERANCE_LENGTH)\n",
    "print(\"Vocabulary and pairs created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279719d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batchToTrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a516f",
   "metadata": {},
   "source": [
    "## Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4061e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- VARS. \n",
    "\n",
    "# Loading vars. \n",
    "MODEL_NAME = 'cb_model'\n",
    "ATTENTION_METHOD = 'dot'\n",
    "HIDDEN_SIZE = 500 \n",
    "ENCODER_N_LAYERS = 2 \n",
    "DECODER_N_LAYERS = 2 \n",
    "DROPOUT = 0.1 \n",
    "BATCH_SIZE = 64\n",
    "checkpoint = dict()\n",
    "\n",
    "# Training vars.\n",
    "SAVE_DIR = os.path.join('data','save') \n",
    "CLIP = 50.0 \n",
    "TEACHER_FORCING_RATIO = 1.0 \n",
    "LEARNING_RATE = 0.0001\n",
    "DECODER_LEARNING_RATIO = 5.0 \n",
    "N_ITERATIONS = 10000\n",
    "PRINT_EVERY = 50\n",
    "SAVE_EVERY = 500\n",
    "TRAIN = True\n",
    "LOAD = False\n",
    "\n",
    "print(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e026dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the saved model path \n",
    "# NOTE: THe checkpoint iter should be defined manually\n",
    "CHECKPOINT_ITER = 15_000\n",
    "SAVED_MODEL_PATH = os.path.join(\n",
    "                SAVE_DIR, MODEL_NAME, CORPUS_NAME,\n",
    "                \"{}-{}_{}\".format(\n",
    "                ENCODER_N_LAYERS, DECODER_N_LAYERS, HIDDEN_SIZE),\n",
    "                '{}_checkpoint.tar'.format(CHECKPOINT_ITER))\n",
    "\n",
    "print(SAVED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb589894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize layers, models, and optimizers  \n",
    "print(\"Initializing models...\")\n",
    "embedding = nn.Embedding(voc.num_words, HIDDEN_SIZE)\n",
    "encoder = EncoderRNN(\n",
    "    HIDDEN_SIZE, embedding, ENCODER_N_LAYERS, DROPOUT)\n",
    "decoder = LuongAttnDecoderRNN(\n",
    "    ATTENTION_METHOD, embedding, HIDDEN_SIZE, voc.num_words, \n",
    "    DECODER_N_LAYERS, DROPOUT)\n",
    "# Initialize optimizers \n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=LEARNING_RATE * DECODER_LEARNING_RATIO)\n",
    "print(\"Models initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a74c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from the saved model file if it exists \n",
    "checkpoint = {}\n",
    "if LOAD:\n",
    "    checkpoint = loadModel(voc, SAVED_MODEL_PATH, embedding, \n",
    "          encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
    "if not 'iteration' in checkpoint:\n",
    "    print(\"No saved model found\")\n",
    "    checkpoint = {'iteration' : 0} \n",
    "else:\n",
    "    print(\"Loading from saved checkpoint: {}\".format(SAVED_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae9e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder / decoder should use the right device \n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "# Configure to use Cuda if available\n",
    "try:\n",
    "    for state in encoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "    for state in decoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "    print(\"Optimizers using cuda!\")\n",
    "except:\n",
    "    print(\"Optimizers not using cuda!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1996df93",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed798ec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note training will override an existing model that was loaded\n",
    "if TRAIN:\n",
    "    # Put the encoder and decoder in train mode\n",
    "    # NOTE: Without the encoder and decoder in train mode, the \n",
    "    # training is **very** slow. This is important because \n",
    "    # it tells layers such as dropout to behave differently. \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    print(\"Training...\")\n",
    "    train_iters(device, encoder, decoder, encoder_optimizer, \n",
    "                      decoder_optimizer, voc, pairs, BATCH_SIZE, \n",
    "                     N_ITERATIONS, SAVED_MODEL_PATH, checkpoint, CLIP, \n",
    "                     PRINT_EVERY, SAVE_EVERY, SAVE_DIR, MODEL_NAME, \n",
    "                     CORPUS_NAME, ENCODER_N_LAYERS, DECODER_N_LAYERS, \n",
    "                     HIDDEN_SIZE, embedding)\n",
    "    print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27b944",
   "metadata": {},
   "source": [
    "## Language generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e601224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting deropout layers for model to eval mode. \n",
    "# NOTE: Putting the encoder / decoder in eval mode tells them \n",
    "# to not use layers such as dropout which are more important \n",
    "# in training. \n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder,device)\n",
    "\n",
    "# evaluateInput(searcher, voc, MAX_UTTERANCE_LENGTH , device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c36fcd5",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the RL training parameters \n",
    "# NOTE: Most of the parameters have been previously defined \n",
    "# and will be reused here. \n",
    "\n",
    "# Loading vars. \n",
    "MODEL_NAME = 'rl_model_seq'\n",
    "ATTENTION_METHOD = 'dot'\n",
    "HIDDEN_SIZE = 500 \n",
    "ENCODER_N_LAYERS = 2 \n",
    "DECODER_N_LAYERS = 2 \n",
    "DROPOUT = 0.1 \n",
    "BATCH_SIZE = 64\n",
    "checkpoint = dict()\n",
    "\n",
    "# Training vars.\n",
    "SAVE_DIR = os.path.join('data','save') \n",
    "CLIP = 50.0 \n",
    "TEACHER_FORCING_RATIO = 0.5\n",
    "LEARNING_RATE = 0.0001\n",
    "DECODER_LEARNING_RATIO = 5.0 \n",
    "N_ITERATIONS = 500\n",
    "PRINT_EVERY = 1\n",
    "SAVE_EVERY = 1\n",
    "QUERY_EVERY = 1 \n",
    "TRAIN = True\n",
    "LOAD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c49810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, we now have a trained encoder and decoder \n",
    "# that we want to train further using the reinforcement learning \n",
    "# mechanism. \n",
    "\n",
    "# NOTE: The forward encoder / decoder are trained.\n",
    "forward_encoder = encoder\n",
    "forward_decoder = decoder\n",
    "forward_encoder = forward_encoder.to(device)\n",
    "forward_decoder = forward_decoder.to(device)\n",
    "\n",
    "# NOTE: The backward encoder / decoder are NOT trained. \n",
    "backward_encoder = EncoderRNN(\n",
    "    HIDDEN_SIZE, embedding, ENCODER_N_LAYERS, DROPOUT)\n",
    "backward_decoder = LuongAttnDecoderRNN(\n",
    "    ATTENTION_METHOD, embedding, HIDDEN_SIZE,\n",
    "    voc.num_words, DECODER_N_LAYERS, DROPOUT)\n",
    "backward_encoder = backward_encoder.to(device)\n",
    "backward_decoder = backward_decoder.to(device)\n",
    "\n",
    "# Initializing the optimizers \n",
    "forward_encoder_optimizer = optim.Adam(forward_encoder.parameters(), lr=LEARNING_RATE)\n",
    "forward_decoder_optimizer = optim.Adam(forward_decoder.parameters(), lr=LEARNING_RATE * DECODER_LEARNING_RATIO)\n",
    "backward_encoder_optimizer = optim.Adam(backward_encoder.parameters(), lr=LEARNING_RATE)\n",
    "backward_decoder_optimizer = optim.Adam(backward_decoder.parameters(), lr=LEARNING_RATE * DECODER_LEARNING_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The checkpoint iter should be defined manually\n",
    "CHECKPOINT_ITER = 5000\n",
    "SAVED_MODEL_PATH = os.path.join(\n",
    "                SAVE_DIR, MODEL_NAME, CORPUS_NAME,\n",
    "                \"{}-{}_{}\".format(\n",
    "                ENCODER_N_LAYERS, DECODER_N_LAYERS, HIDDEN_SIZE),\n",
    "                '{}_checkpoint.tar'.format(CHECKPOINT_ITER))\n",
    "\n",
    "print(SAVED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911fd025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from the saved model file if it exists \n",
    "# NOTE: Assuming we need to load the forward encoder / decoder. \n",
    "checkpoint = {}\n",
    "if LOAD:\n",
    "    checkpoint = loadModel(voc, SAVED_MODEL_PATH, embedding, \n",
    "            forward_encoder, forward_decoder, forward_encoder_optimizer, \n",
    "            forward_decoder_optimizer)\n",
    "if not 'iteration' in checkpoint:\n",
    "    print(\"No saved model found\")\n",
    "    checkpoint = {'iteration' : 0} \n",
    "else:\n",
    "    print(\"Loading from saved checkpoint: {}\".format(SAVED_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722ea919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizer parameters to the right device. \n",
    "try:\n",
    "    # If you have cuda, configure cuda to call\n",
    "    for state in forward_encoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "    for state in forward_decoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "    for state in backward_encoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "    for state in backward_decoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "    print(\"Optimizers using cuda!\")\n",
    "except:\n",
    "    print(\"Optimizers not using cuda!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc5ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the human trainer \n",
    "\n",
    "HUMAN_METRICS = [\n",
    "    'How many of the above sentences are easy to respond to?',\n",
    "    'How many of the above sentences contribute new information?',\n",
    "    'How many of the above sentences have good grammar?'\n",
    "]\n",
    "\n",
    "# This gets passed into rl_iters for training. \n",
    "human_trainer = HumanTrainer(HUMAN_METRICS, QUERY_EVERY, voc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ded4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training loop \n",
    "\n",
    "# TODO: The arguments here are not correct. \n",
    "if TRAIN:\n",
    "    # Set the encoders / decoders to training mode for the dropout \n",
    "    # layers to be used. \n",
    "    forward_encoder.train()\n",
    "    forward_decoder.train()\n",
    "    backward_encoder.train()\n",
    "    backward_decoder.train()\n",
    "    \n",
    "    rl_iters(\n",
    "        device, forward_encoder, forward_encoder_optimizer, \n",
    "        forward_decoder, forward_decoder_optimizer, \n",
    "        backward_encoder, backward_encoder_optimizer, \n",
    "        backward_decoder, backward_decoder_optimizer, \n",
    "        voc, pairs, BATCH_SIZE, TEACHER_FORCING_RATIO, \n",
    "        DULL_RESPONSES, N_ITERATIONS, PRINT_EVERY, \n",
    "        SAVE_EVERY, QUERY_EVERY, SAVE_DIR, checkpoint, \n",
    "        SAVED_MODEL_PATH,\n",
    "        MIN_TRIM_COUNT, MAX_UTTERANCE_LENGTH, MODEL_NAME, \n",
    "        CORPUS_NAME, ENCODER_N_LAYERS, HIDDEN_SIZE, \n",
    "        DECODER_N_LAYERS, embedding, human_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a076a709",
   "metadata": {},
   "source": [
    "## Optimized Language Generation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a87892",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_encoder.eval()\n",
    "forward_decoder.eval()\n",
    "\n",
    "# NOTE: The final model is not the TRAINED encoder / decoder \n",
    "searcher = GreedySearchDecoder(forward_encoder, forward_decoder,device)\n",
    "\n",
    "evaluateInput(searcher, voc, MAX_UTTERANCE_LENGTH , device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e33016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "808ff6615bcef11d407db924ddf84c8596836616e1fc1634678029648ad60f84"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
