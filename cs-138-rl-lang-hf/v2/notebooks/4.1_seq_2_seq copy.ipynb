{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal in this notebook is to train a sequence to sequence model with \n",
    "attention on the OpenSubtitles Dataset, with the goal of using each \n",
    "turn in the dataset as a target and the concatenation of the two previous \n",
    "sentences as the source inputs. \n",
    "\n",
    "The goal of the model is to predict next sentences based on previous sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import sklearn \n",
    "assert sklearn.__version__ >= \"0.20\" \n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "import tensorflow_text as tf_text \n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "# Others \n",
    "import transformers \n",
    "\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing te Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the daily dialogue dataset to train the language model. \n",
    "\n",
    "https://arxiv.org/abs/1710.03957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets, load_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset daily_dialog (/Users/muhammadumair/.cache/huggingface/datasets/daily_dialog/default/1.0.0/c03444008e9508b8b76f1f6793742d37d5e5f83364f8d573c2747bff435ea55c)\n",
      "100%|██████████| 3/3 [00:00<00:00, 269.98it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('daily_dialog' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_full = dataset[\"train\"]['dialog'] \n",
    "test_dataset_full = dataset[\"test\"]['dialog'] \n",
    "val_dataset_full = dataset[\"validation\"]['dialog'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining the daily dialogue dataset, where every item is a conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Say , Jim , how about going for a few beers after dinner ? ',\n",
       " ' You know that is tempting but is really not good for our fitness . ',\n",
       " ' What do you mean ? It will help us to relax . ',\n",
       " \" Do you really think so ? I don't . It will just make us fat and act silly . Remember last time ? \",\n",
       " \" I guess you are right.But what shall we do ? I don't feel like sitting at home . \",\n",
       " ' I suggest a walk over to the gym where we can play singsong and meet some of our friends . ',\n",
       " \" That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them . \",\n",
       " ' Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too . ',\n",
       " \" Good.Let ' s go now . \",\n",
       " ' All right . ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_full[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "\n",
    "def daily_dialogue_preprocess(dataset): \n",
    "    X = list()\n",
    "    for item in dataset: \n",
    "        X_conv = ['',item[0]]\n",
    "        for i in range(1, len(item) -1): \n",
    "            X_conv.append(item[i-1] + item[i])  \n",
    "        X.append(X_conv)\n",
    "    y = [item for item in dataset] \n",
    "    # Flatten the list \n",
    "    X = list(itertools.chain(*X))\n",
    "    y = list(itertools.chain(*y) ) \n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, y_train_full = daily_dialogue_preprocess(train_dataset_full) \n",
    "X_val_full, y_val_full = daily_dialogue_preprocess(val_dataset_full) \n",
    "X_test_full, y_test_full = daily_dialogue_preprocess(test_dataset_full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'Say , Jim , how about going for a few beers after dinner ? '),\n",
       " ('Say , Jim , how about going for a few beers after dinner ? ',\n",
       "  ' You know that is tempting but is really not good for our fitness . '),\n",
       " ('Say , Jim , how about going for a few beers after dinner ?  You know that is tempting but is really not good for our fitness . ',\n",
       "  ' What do you mean ? It will help us to relax . ')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(X_train_full[:3], y_train_full[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_train_full) == len(y_train_full) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87170"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a word level tokenizer to tokenize all the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(text):\n",
    "    \"\"\"\n",
    "    This method standardizes the text in each sentence. \n",
    "    \"\"\"\n",
    "    # Split accecented characters.\n",
    "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "    # Keep space, a to z, and select punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "    # Add spaces around punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "    # Strip whitespace.\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-27 12:40:21.658048: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = 5000\n",
    "output_sequence_length= 100\n",
    "\n",
    "input_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=preprocess_sentence, \n",
    "    max_tokens=max_vocab_size, \n",
    "    output_sequence_length=output_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_processor.adapt(X_train_full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Do you really think so ? I don't . It will just make us fat and act silly . Remember last time ?  I guess you are right.But what shall we do ? I don't feel like sitting at home . \"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch = [X_train_full[5]] \n",
    "example_input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' I suggest a walk over to the gym where we can play singsong and meet some of our friends . ']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_output_batch = [y_train_full[5]] \n",
    "example_output_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 100])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization of the example batch. \n",
    "example_tokens = input_text_processor(example_input_batch)\n",
    "example_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START] do you really think so ? i dont . it will just make us fat and act silly . remember last time ? i guess you are right . but what shall we do ? i dont feel like sitting at home . [END]                                                       '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also convert the tokens back using the vocabulary. \n",
    "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_data(X,y): \n",
    "#     X = input_text_processor(X)\n",
    "#     y = input_text_processor(y) \n",
    "#     return X,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_tok, y_train_tok = tokenize_data(X_train_full,y_train_full) \n",
    "# X_val_tok, y_val_tok = tokenize_data(X_val_full,y_val_full) \n",
    "# X_val_tok, y_val_tok = tokenize_data(X_test_full,y_test_full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert X_train_tok.shape[0] == y_train_tok.shape[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X,y, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X,y)) \n",
    "    dataset = dataset.shuffle(len(X)).batch(batch_size)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(X_train_full, y_train_full) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Model with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super().__init__()\n",
    "        self.enc_units = enc_units \n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            self.input_vocab_size, embedding_dim) \n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            self.enc_units, return_sequences=True, return_state=True, \n",
    "            recurrent_initializer=\"glorot_uniform\") \n",
    "        \n",
    " \n",
    "    def call(self, tokens, state=None):\n",
    "        vectors = self.embedding(tokens) # Ret shape: (batch, s, embedding_dim)\n",
    "        #    output shape: (batch, s, enc_units)\n",
    "        #    state shape: (batch, enc_units)\n",
    "        output, state = self.gru(vectors, initial_state=state) \n",
    "        return output, state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super().__init__() \n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False) \n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False) \n",
    "        self.attention = tf.keras.layers.AdditiveAttention() \n",
    "\n",
    "    def call(self, query, value, mask):\n",
    "        w1_query = self.W1(query)\n",
    "        w2_key = self.W2(value)\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs = [w1_query, value, w2_key],\n",
    "            mask=[query_mask, value_mask],\n",
    "            return_attention_scores = True,\n",
    "        )\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "        super().__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # For Step 1. The embedding layer convets token IDs to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            self.output_vocab_size, embedding_dim)\n",
    "        # For Step 2. The RNN keeps track of what's been generated so far\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            self.dec_units, return_sequences=True, return_state=True, \n",
    "            recurrent_initializer=\"glorot_uniform\")\n",
    "        # For step 3. The RNN output will be the query for the attention layer.\n",
    "        self.attention = BahdanauAttention(self.dec_units) \n",
    "        # For step 4. Eqn. (3): converting `ct` to `at`\n",
    "        self.Wc = tf.keras.layers.Dense(\n",
    "            dec_units, activation=tf.math.tanh,use_bias=False) \n",
    "        # For step 5. This fully connected layer produces the logits for each\n",
    "        # output token.\n",
    "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
    "\n",
    "    def call(self, inputs, state=None):\n",
    "        # state shape: (batch, dec_units)\n",
    "        # new_tokens_shape: (batch, t)\n",
    "        # enc_output shape: (batch, s, enc_units) \n",
    "        new_tokens, enc_output, mask = inputs  \n",
    "\n",
    "        vectors = self.embedding(new_tokens) \n",
    "        rnn_output, state = self.gru(vectors, initial_state=state)\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            query=rnn_output, value=enc_output, mask=mask)\n",
    "        context_and_rnn_output = tf.concat([context_vector,rnn_output],axis=-1)\n",
    "        attention_vector = self.Wc(context_and_rnn_output)\n",
    "        logits = self.fc(attention_vector) \n",
    "        return (logits, attention_weights), state \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2Seq(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, embedding_dim, units, input_text_processor, \n",
    "            output_text_processor):\n",
    "        super().__init__()\n",
    "        encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                      embedding_dim, units)\n",
    "        decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                        embedding_dim, units)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        return self._train_step(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(self, input_text, target_text):\n",
    "    input_tokens = self.input_text_processor(input_text) \n",
    "    target_tokens = self.output_text_processor(target_text) \n",
    "    input_mask = input_tokens != 0 \n",
    "    target_mask = target_tokens != 0 \n",
    "    return input_tokens, input_mask, target_tokens, target_mask \n",
    "\n",
    "AttentionSeq2Seq._preprocess = _preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_step(self, inputs):\n",
    "    input_text, target_text = inputs \n",
    "    (input_tokens, input_mask,\n",
    "        target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
    "    max_target_length = tf.shape(target_tokens)[1]\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Encode the input\n",
    "        enc_output, enc_state = self.encoder(input_tokens)\n",
    "\n",
    "        # Initialize the decoder's state to the encoder's final state.\n",
    "        # This only works if the encoder and decoder have the same number of\n",
    "        # units.\n",
    "        dec_state = enc_state\n",
    "        loss = tf.constant(0.0)\n",
    "\n",
    "        for t in tf.range(max_target_length-1):\n",
    "            # Pass in two tokens from the target sequence:\n",
    "            # 1. The current input to the decoder.\n",
    "            # 2. The target for the decoder's next prediction.\n",
    "            new_tokens = target_tokens[:, t:t+2]\n",
    "            step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
    "                                                    enc_output, dec_state)\n",
    "            loss = loss + step_loss\n",
    "\n",
    "        # Average the loss over all non padding tokens.\n",
    "        average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "\n",
    "    # Apply an optimization step\n",
    "    variables = self.trainable_variables \n",
    "    gradients = tape.gradient(average_loss, variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    # Return a dict mapping metric names to current value\n",
    "    return {'batch_loss': average_loss}\n",
    "\n",
    "\n",
    "AttentionSeq2Seq._train_step = _train_step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "  input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
    "  # Run the decoder one step.\n",
    "  decoder_input = (input_token, enc_output, input_mask)\n",
    "  dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
    "  logits, attention_weights = dec_result\n",
    "  # `self.loss` returns the total for non-padded tokens\n",
    "  y = target_token\n",
    "  y_pred = logits\n",
    "  step_loss = self.loss(y, y_pred)\n",
    "  return step_loss, dec_state\n",
    "\n",
    "AttentionSeq2Seq._loop_step = _loop_step "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "  def __init__(self):\n",
    "    self.name = 'masked_loss'\n",
    "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "\n",
    "  def __call__(self, y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss = self.loss(y_true, y_pred)\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    loss *= mask\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128 \n",
    "UNITS = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_2_seq = AttentionSeq2Seq(\n",
    "    EMBEDDING_DIM, UNITS, input_text_processor,input_text_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_2_seq.compile(\n",
    "    optimizer= tf.optimizers.Adam(), \n",
    "    loss=MaskedLoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.129977>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.123968>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.117443>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.109748>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.100172>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.087882>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.071827>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.050645>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.022541>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.9851103>}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Test to make sure that the training loop is working. \n",
    "# for n in range(10):\n",
    "#   print(seq_2_seq.train_step([example_input_batch, example_output_batch]))\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLogs(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, key):\n",
    "    self.key = key\n",
    "    self.logs = []\n",
    "\n",
    "  def on_train_batch_end(self, n, logs):\n",
    "    self.logs.append(logs[self.key])\n",
    "\n",
    "batch_loss = BatchLogs('batch_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Would you tell me how I should send this parcel to Shanghai , China ? It contains only books . '\n",
      " b\" Sorry , madam . I afraid you have a wrong number.we don't have Mr Over here .  I want 6420422 3 , is that right ? \"\n",
      " b\" well , that's true .  how about your mother ? \"\n",
      " b'What channel did you watch last night ? ' b''], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b\" You might send it as'Printed Matter ' . \"\n",
      " b' No , you give a wrong number . '\n",
      " b' she also believes in healthy diet . And she requires us to have regular meals . '\n",
      " b' Channel Two . A TV series was showing on it . The name of the series is Huanzhu Gene '\n",
      " b\"What's the weather like ? \"], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_input_batch, example_target_batch in train_dataset.take(1):\n",
    "  print(example_input_batch[:5])\n",
    "  print()\n",
    "  print(example_target_batch[:5])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"/Users/muhammadumair/anaconda3/envs/dl_lang/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/muhammadumair/anaconda3/envs/dl_lang/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/muhammadumair/anaconda3/envs/dl_lang/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/var/folders/96/83knfb594fb5jg1ptk9m5xhc0000gn/T/ipykernel_13891/4095551697.py\", line 16, in train_step\n        return self._train_step(inputs)\n    File \"/var/folders/96/83knfb594fb5jg1ptk9m5xhc0000gn/T/ipykernel_13891/936795159.py\", line 16, in _train_step\n        for t in tf.range(max_target_length-1):\n\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m/Users/muhammadumair/Documents/Repositories/mumair01-repos/RL-LANG-HF/v2/notebooks/4.1_seq_2_seq copy.ipynb Cell 53'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/muhammadumair/Documents/Repositories/mumair01-repos/RL-LANG-HF/v2/notebooks/4.1_seq_2_seq%20copy.ipynb#ch0000048?line=0'>1</a>\u001b[0m seq_2_seq\u001b[39m.\u001b[39;49mfit(train_dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/muhammadumair/Documents/Repositories/mumair01-repos/RL-LANG-HF/v2/notebooks/4.1_seq_2_seq%20copy.ipynb#ch0000048?line=1'>2</a>\u001b[0m                      callbacks\u001b[39m=\u001b[39;49m[batch_loss])\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_lang/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///~/anaconda3/envs/dl_lang/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/anaconda3/envs/dl_lang/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///~/anaconda3/envs/dl_lang/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///~/anaconda3/envs/dl_lang/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///~/anaconda3/envs/dl_lang/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_lang/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/dl_lang/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/dl_lang/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///~/anaconda3/envs/dl_lang/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/dl_lang/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/dl_lang/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1148'>1149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"/Users/muhammadumair/anaconda3/envs/dl_lang/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/muhammadumair/anaconda3/envs/dl_lang/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/muhammadumair/anaconda3/envs/dl_lang/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/var/folders/96/83knfb594fb5jg1ptk9m5xhc0000gn/T/ipykernel_13891/4095551697.py\", line 16, in train_step\n        return self._train_step(inputs)\n    File \"/var/folders/96/83knfb594fb5jg1ptk9m5xhc0000gn/T/ipykernel_13891/936795159.py\", line 16, in _train_step\n        for t in tf.range(max_target_length-1):\n\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n"
     ]
    }
   ],
   "source": [
    "seq_2_seq.fit(train_dataset, epochs=3,\n",
    "                     callbacks=[batch_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1fd79731c1c876475fd69ebbaebe6a3e94c183f58b856f4def482a2ffc9ff81c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('diag-gen')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
